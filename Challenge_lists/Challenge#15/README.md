# Challenge #15
**Eric Zhou**  
**April 30, 2025**

# introduction
in this project we are going to try to map our algurithm to systemverilog to achieve hardware acceleration. However, as I am a new nerd to HDL, I heavily used chatgpt as a reference in this challenge, from code Block plan, software too choice and installation and coding itself.

In challenge 15, I would like to map my gpt2 algurithm into HDL. after above digging into the code, I thik I am ready to pose the code diagram
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Model Config    â”‚
â”‚ GPTConfig:         â”‚
â”‚  - block_size=1024 â”‚
â”‚  - vocab_size=50304â”‚
â”‚  - n_layer=12      â”‚
â”‚  - n_head=12       â”‚
â”‚  - n_embd=768      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Model Constructionâ”‚
â”‚  - Token Embedding   â”‚
â”‚  - Positional Embed  â”‚
â”‚  - Transformer Blocksâ”‚
â”‚      â€¢ LayerNorm     â”‚
â”‚      â€¢ SelfAttention â”‚
â”‚      â€¢ MLP + Res     â”‚
â”‚  - Final LayerNorm   â”‚
â”‚  - Linear lm_head    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Load Pretrained Weights  â”‚
â”‚  - Match with HF checkpoint â”‚
â”‚  - Transpose where needed   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Inference Loop (generate)  â”‚
â”‚  Input Prompt â†’ Encode (token IDs) â”‚
â”‚  For each token to generate:       â”‚
â”‚   - Crop context if too long       â”‚
â”‚   - Forward through model          â”‚
â”‚   - Predict next token probs       â”‚
â”‚   - Sample next token              â”‚
â”‚   - Append to prompt               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Decode Token Sequence â”‚
â”‚  - Convert back to text  â”‚
â”‚  - Output final string   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

We start by building a Transformer-based language model with GPTConfig â€” specifying depth, width, and vocabulary size.
Then we initialize the model (GPT), which contains embedding layers, a stack of Transformer blocks (each with LayerNorm, Attention, MLP), and a projection head.
We then load pretrained weights from Hugging Face into our custom model.
Once ready, we feed in a prompt, and in each iteration of the generation loop:

We crop context (if needed), forward it through the model, and get token probabilities.

We sample one token, append it to the input, and repeat until we reach the desired output length.
Finally, we decode the token IDs back into text.

above is our previous analysis

and below is your previous thoughts 

My scope in this part is to build  hardware acceleration for the gpt2 model.
I will focus on GPT-2 124M inference only, with fixed weights, and explore HDL implementation. 
ğŸ¯ Model: GPT-2 124M (n_layer=12, n_head=12, n_embd=768)
- Fixed weights â€” no training or backpropagation

- Inference only (no gradient computation)

- Full model mapped to hardware (ideal), or block-by-block in reality


Full GPT-2 HDL Block-Level Architecture (Inference Only)!!!

I first ask gpt advice for HDL plan:

âš™ï¸ HDL Module Plan (RTL Perspective)

HDL Module	Description	Estimated Complexity
token_embedding.v	ROM + lookup	Low
pos_embedding.v	ROM + adder	Low
qkv_linear.v	Fixed-weight matrix-vector product (3x)	High
attention_core.v	QKáµ€ â†’ softmax â†’ weighted sum (V)	Very High
mlp_ffn.v	Linear + GELU + Linear	High
layernorm.v	Mean/Var normalize	Medium (LUT-approx optional)
final_linear.v	Linear projection to vocab	High
softmax.v	Exp + Normalize	Medium (LUT-based or CORDIC)
controller.v	Token FSM + Layer scheduler	Medium



I paste my above work to chatgpt and it help me to generate  Block-Level HDL Mapping for verilog
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Module: GPT2_TOP.v                         â”‚
â”‚ - Controls token stream & orchestrates     â”‚
â”‚   token generation steps                   â”‚
â”‚ - Interfaces with memory for embeddings    â”‚
â”‚ - Connects Transformer blocks sequentially â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [1] Token & Positional Embeddings          â”‚
â”‚ token_embedding.v: ROM lookup (token ID â†’ vector)   â”‚
â”‚ pos_embedding.v: ROM lookup + add (position â†’ vector) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [2] Transformer Block Ã— 12 (pipelined or FSM) â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ layernorm.v (x2)                        â”‚ â”‚
â”‚ â”‚ qkv_linear.v                            â”‚ â”‚
â”‚ â”‚ attention_core.v                        â”‚ â”‚
â”‚ â”‚ residual_add.v                          â”‚ â”‚
â”‚ â”‚ mlp_ffn.v (Linear â†’ GELU â†’ Linear)      â”‚ â”‚
â”‚ â”‚ residual_add.v                          â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [3] Final LayerNorm & Linear Head          â”‚
â”‚ final_layernorm.v                          â”‚
â”‚ final_linear.v (projection to vocab)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [4] Softmax + Sampling                     â”‚
â”‚ softmax.v (LUT or CORDIC-based exp/log)    â”‚
â”‚ sampling.v (argmax or random + CDF)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [5] Controller.v                           â”‚
â”‚ FSM:                                       â”‚
â”‚  - Manages token iteration loop            â”‚
â”‚  - Schedules each Transformer layer        â”‚
â”‚  - Crops context if needed (fixed buffer)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ğŸ“¦ Module Breakdown and HDL Notes
HDL Module	Description	HDL Strategy
token_embedding.v	Token ID â†’ Embedding Vec (ROM lookup)	Preload weights in BRAM
pos_embedding.v	Position ID â†’ Pos Vec + Add	Offset adder, ROM
layernorm.v	Normalize to mean/std	Use LUT/log approximations
qkv_linear.v	3Ã— Matrix-Vector Multiply (Q,K,V)	Use MAC array (systolic or time-muxed)
attention_core.v	QKáµ€ â†’ softmax â†’ V	Heavy: matrix mult + softmax + MAC
mlp_ffn.v	Linear â†’ GELU â†’ Linear	Use ReLU/GELU LUT, two FCs
final_linear.v	Projection to vocab size (50304)	Partial output, tiled, or compressed
softmax.v	Normalization	Approximate via exp LUT + div
sampling.v	Next token sampling	Argmax or CDF with RNG
controller.v	FSM to control pipeline	Handles context window, prompt token loop


Suffering:
At first I want to follow the python tool to write verilog and do validation. But I find the specicate verilator tool version the pyMTL3 asked for is a pain to install. After I rebuild in Linux. Some strange error poped up. I also search such version in Github, there is a discussion out there indicates that we are not able to use such version.
Then I move forward, I may just want to write pure verilog with latest version of verilator.

My first choice of valication tool is verilator. It has been a pain for me to first install it as at first. It would be useful to just rebuild that

verilator generate wrong results

change to Questa

rewrite in verilog

data quantalization 


`timescale 1ns/100ps    //æ—¶é—´å•ä½ä¸º1nsï¼Œç²¾åº¦ä¸º100psï¼Œåˆæ³•
shiji